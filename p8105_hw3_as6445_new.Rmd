---
title: "p8105_hw3_as6445"
author: "Ayako Sekiya"
date: "2022-10-07"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r library}
library(tidyverse)
library(ggridges)
library(patchwork)

library(p8105.datasets)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

#### Read into data

```{r read data}
data("instacart")

instacart = 
  instacart %>% 
  as_tibble(instacart)
```

There are `r nrow(instacart)` observations and `r ncol(instacart)` columns in the instacart data. The variables included are as following: `r colnames(instacart)`.  

Below is a table summarizing the number of items ordered from aisle. In total, there are 134 aisles, with fresh vegetables and fresh fruits holding the most items ordered by far.

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))

```

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 2)
```

## Problem 2

```{r tidy accel}
accel = 
  read_csv(file = "./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440, 
    names_to = "activity",
    values_to = "activity_count") %>% 
  mutate(day_type= ifelse(day=='Saturday','Weekend',
                   ifelse(day=='Sunday','Weekend','Weekday')))
      
```
There are `r nrow(accel)` observations and `r ncol(accel)` columns in the Accel data. The variables included are as following: `r colnames(accel)`.

### Using your tidied dataset, aggregate across minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?

```{r analysis}
accel_analysis = accel %>% 
  group_by(week,day_id) %>% 
  summarize(sum_totalactivity = sum(activity_count)) %>% 
  ggplot() +
  geom_bar(aes(x = day_id, y = sum_totalactivity, fill= week),stat="identity")+
  scale_x_continuous(breaks = c(0, 7, 14, 21, 28, 35),
                   labels=c("0", "7", "14", "21", "28", "35"))+
  theme(legend.position = "right")
  
accel_analysis
```

Based on this plot, this person seems to be more active towards the end of the week around the weekends. The person seems to be decently active during the middle of the week, but the level of activity does overall seem lower. 

### Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.

```{r aggregate}
accel_24 = accel %>% 
  select (day, activity, activity_count, week) %>% 
  ggplot(aes(x =activity , y = activity_count, color=day)) + 
  geom_point()+
   geom_line(alpha = 0.5 ) +  
  labs(
    title = "24-hour activity time for each day by week",
    x = "day",
    y = "activity count (min))",
    caption = "Data from accel data") +
  theme(plot.title = element_text(size = 10, face = "bold"),
        legend.position = "right",
        legend.text = element_text(size = 7))+
  scale_x_discrete(breaks = c(1980, 1985, 1990, 1995, 2000, 2005, 2010)) +
  theme(plot.title = element_text(size = 10, face = "bold"),
        legend.position = "right",
        legend.text = element_text(size = 7))  

accel_24 

ggsave("accel_24.pdf")
```

## Problem 3

#### Read into data

```{r read data}
data("ny_noaa")
```

We will first tidy the dataset!

```{r tidy noaa}
ny_noaa<-
  ny_noaa %>% 
  janitor::clean_names() %>%
  separate(date, into=c("year", "month", "day"), sep= "-") %>% 
  mutate(tmax = as.numeric(tmax, na.rm = TRUE)/10,
         tmin = as.numeric(tmin, na.rm = TRUE)/10,
        prcp = as.numeric(prcp, na.rm = TRUE)/10,
        month= as.numeric(month)) %>% 
  mutate(month = month.name[month])
```
   
#### For snowfall, what are the most commonly observed values? Why?

To answer this question, we will use `group_by` and `summarize`. 

```{r snowfall}
snow=ny_noaa %>% 
  group_by(snow) %>% 
  summarize(n_obs = n())
```

The most commonly observed values is 0 based on this output which would make sense because there should be more days that it does not snow in NY than days that snows. 

#### Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

```{r}
avg_jan_july_p<-
  ny_noaa %>%
  select( month, id, year, tmax) %>%
  filter(month=='January'|month=='July') %>%
  group_by(id, year, month) %>% 
  summarise(mean_tmax = mean(tmax)) %>%
  drop_na(mean_tmax)%>%
  ggplot(aes(x = year, y = mean_tmax)) +
  geom_point(aes(color=id, alpha = .1))+
  theme(legend.position = "bottom",
        legend.key.size = unit(0.05, "cm"), 
        legend.text = element_text(size = 7),
        legend.direction = "horizontal") + 
  labs(
    title = "Average max temps in January and July",
    x = "Years",
    y = "Average Max daily temperature (C)",
    caption = "Data from the ny noaa package") +
  scale_x_discrete(breaks = c(1980, 1985, 1990, 1995, 2000, 2005, 2010),
      labels = c("1980","1985", "1990", "1995", "2000", "2005", "2010")) +
  facet_grid(~month)

avg_jan_july_p

ggsave("avg_jan_july_p.pdf")
```

### Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option). 

```{r two-panel tmin and max}
tmax_tmin_p = ny_noaa %>% 
  select(tmax, tmin, year) %>% 
  ggplot(aes(x = tmax, y = tmin)) + 
  geom_hex() + 
  labs(
    title = "Temperature max/ minplot",
    x = "Maximum daily temperature (C)",
    y = "Minimum daily temperature (C)",
    caption = "Data from the ny noaa package")+
  scale_x_continuous(
    breaks = c(-30, -15, 0, 15, 30, 45, 60), 
    labels = c("-30°C","-15°C", "0°C", "15°C", "30°C", "45°C", "60°C")) +
  scale_y_continuous(
    breaks = c(-60, -45, -30, -15, 0, 15, 30, 45, 60), 
    labels = c("-60°C","-45°C","-30°C","-15°C", "0°C", "15°C", "30°C", "45°C", "60°C")) 

tmax_tmin_p

ggsave("tmax_tmin_p.pdf")
```


From this plot, we can see that there are the highest concentrations where there is the lightest color. There seems to be a correlation between minimum and maximum daily temperatures around where there is the most concentrated data. 

### Make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r}
snow_plot= ny_noaa %>% 
  select (year, snow) %>% 
  filter(snow > 0 & snow < 100) %>% 
  mutate(year = as.character(year)) %>% 
  ggplot(aes(x = year, y = snow, fill = year)) + 
  geom_violin(alpha = 0.5 ) +  
  labs(
    title = "Snowfall distribution (0-100mm) by year",
    x = "Year",
    y = "Snowfall (mm)",
    caption = "Data from the ny noaa package") +
  scale_x_discrete(breaks = c(1980, 1985, 1990, 1995, 2000, 2005, 2010)) +
  theme(plot.title = element_text(size = 10, face = "bold"),
        legend.position = "right",
        legend.text = element_text(size = 7))

snow_plot

ggsave("snow_plot.pdf")
```

From this plot, there seems to be the highest concentration of snowfall between approximately 0-25mm. The data is skewed to the right, and there are fewer occasions where it snowed more than 75mm of snow throughout the years. 
