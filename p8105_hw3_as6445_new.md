p8105_hw3_as6445
================
Ayako Sekiya
2022-10-07

``` r
library(tidyverse)
```

    ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──
    ## ✔ ggplot2 3.3.6      ✔ purrr   0.3.4 
    ## ✔ tibble  3.1.8      ✔ dplyr   1.0.10
    ## ✔ tidyr   1.2.0      ✔ stringr 1.4.1 
    ## ✔ readr   2.1.2      ✔ forcats 0.5.2 
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()

``` r
library(ggridges)
library(p8105.datasets)
data("instacart")
data("ny_noaa")
```

## Problem 1

There are 1384617 observations and 15 columns in the Accel data. The
variables included are as following: order_id, product_id,
add_to_cart_order, reordered, user_id, eval_set, order_number,
order_dow, order_hour_of_day, days_since_prior_order, product_name,
aisle_id, department_id, aisle, department.

## Problem 2

``` r
accel = 
  read_csv(file = "./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440, 
    names_to = "activity",
    values_to = "activity_count") %>% 
  mutate(day_type= ifelse(day=='Saturday','Weekend',
                   ifelse(day=='Sunday','Weekend','Weekday')))
```

    ## Rows: 35 Columns: 1443
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr    (1): day
    ## dbl (1442): week, day_id, activity.1, activity.2, activity.3, activity.4, ac...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

There are 50400 observations and 6 columns in the Accel data. The
variables included are as following: week, day_id, day, activity,
activity_count, day_type.

``` r
accel_analysis = accel %>% 
  group_by(day_type) %>% 
  summarize(sum_totalactivity = sum(activity_count))
```

``` r
accel_24 = accel %>% 
  group_by(day) %>% 
  ggplot(aes(x = , y = tmin, color = name)) + 
  geom_violin(aes(fill = name), alpha = .5) + 
  stat_summary(fun = "median", color = "blue")
  theme(legend.position = "none")
```

    ## List of 1
    ##  $ legend.position: chr "none"
    ##  - attr(*, "class")= chr [1:2] "theme" "gg"
    ##  - attr(*, "complete")= logi FALSE
    ##  - attr(*, "validate")= logi TRUE

separate(activity, into=c(“activity”, “minute”), sep= “\_“) %\>% select
(-activity) %\>% mutate \## Problem 3

We will first tidy the dataset!

``` r
ny_noaa <- ny_noaa %>% 
  janitor::clean_names() %>%
  separate(date, into=c("year", "month", "day"), sep= "-") %>% 
  mutate(tmax = as.numeric(tmax, na.rm = TRUE)/10,
         tmin = as.numeric(tmin, na.rm = TRUE)/10,
        prcp = as.numeric(prcp, na.rm = TRUE)/10) %>% 
  mutate(month= as.character(month.name[month]))
```

For snowfall, what are the most commonly observed values? Why?

To answer this question, we will use `group_by` and `summarize`.

``` r
snow <- ny_noaa %>% 
  group_by(snow) %>% 
  summarize(n_obs = n())
```

The most commonly observed values is 0 based on this output which would
make sense because there should be more days that it does not snow in NY
than days that snows.

``` r
avg_jan_july_p = 
  ny_noaa %>% 
  select(id, month, year, tmax, tmin) %>% 
  filter(month=='January'| month=='July') %>%
  group_by(id, month, year) %>% 
  summarize(mean_tmax = mean(tmax)) %>%
  ggplot(aes(x = year, y = mean_tmax, color = name)) + 
  geom_point(alpha = .5) +
  theme(legend.position = "none")
```

    ## `summarise()` has grouped output by 'id', 'month'. You can override using the
    ## `.groups` argument.

``` r
tmax_tmin_p = 
  ny_noaa %>% 
  ggplot(aes(x = tmax, y = tmin, color = name)) + 
  geom_hex() 
```

Make a plot showing the distribution of snowfall values greater than 0
and less than 100 separately by year.

``` r
snow_plot<- ny_noaa %>% 
  filter(snow > 0 & snow < 100) %>% 
  mutate(year = as.character(year)) %>% 
  ggplot(aes(x = year, y = snow, fill = year)) + 
  geom_violin(alpha = 0.5 )
```
